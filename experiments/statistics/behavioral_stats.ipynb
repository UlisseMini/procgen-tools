{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb489560",
   "metadata": {},
   "source": [
    "This notebook is for taking statistics over thousands of runs, in order to analyze which maze features (e.g. distance to cheese) tend to affect decision-making. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4881a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import procgen_tools\n",
    "except ImportError:\n",
    "    get_ipython().run_line_magic(\n",
    "        magic_name=\"pip\",\n",
    "        line=\"install -U git+https://github.com/ulissemini/procgen-tools\",\n",
    "    )\n",
    "\n",
    "from procgen_tools.utils import setup\n",
    "\n",
    "setup(dl_stats=True)  # create directory structure and download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c479b48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from procgen import ProcgenGym3Env\n",
    "from procgen_tools import maze\n",
    "from procgen_tools.models import load_policy\n",
    "from procgen_tools.metrics import metrics, decision_square \n",
    "from procgen_tools.data_utils import load_episode\n",
    "\n",
    "from IPython import display\n",
    "from glob import glob\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import random \n",
    "from typing import List, Tuple, Any, Dict, Union, Optional\n",
    "\n",
    "import prettytable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff6529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle text formatting\n",
    "def bold_text(text: str):\n",
    "    return \"\\033[1m\" + text + \"\\033[0m\"\n",
    "\n",
    "\n",
    "# In the \"Understanding and controlling a maze-solving policy network\" post, we claimed that the following attributes are important. We'll often bold them in the text.\n",
    "claimed_attributes = [\n",
    "    \"steps_between_cheese_decision_square\",\n",
    "    \"euc_dist_cheese_decision_square\",\n",
    "    \"euc_dist_cheese_top_right\",\n",
    "    \"euc_dist_decision_square_5x5\",\n",
    "]\n",
    "\n",
    "\n",
    "def english_attr(attr: str) -> str:\n",
    "    \"\"\"Maps an attribute to its English name.\"\"\"\n",
    "    if attr == \"steps_between_cheese_5x5\":\n",
    "        return \"Steps between cheese and top-right 5x5\"\n",
    "    elif attr == \"euc_dist_cheese_5x5\":\n",
    "        return \"Euclidean distance between cheese and top-right 5x5\"\n",
    "    elif attr == \"steps_between_decision_square_5x5\":\n",
    "        return \"Steps between decision square and top-right 5x5\"\n",
    "    elif attr == \"euc_dist_decision_square_5x5\":\n",
    "        return \"Euclidean distance between decision square and top-right 5x5\"\n",
    "    elif attr == \"steps_between_cheese_top_right\":\n",
    "        return \"Steps between cheese and top right square\"\n",
    "    elif attr == \"euc_dist_cheese_top_right\":\n",
    "        return \"Euclidean distance between cheese and top right square\"\n",
    "    elif attr == \"steps_between_decision_square_top_right\":\n",
    "        return \"Steps between decision square and top right square\"\n",
    "    elif attr == \"euc_dist_decision_square_top_right\":\n",
    "        return (\n",
    "            \"Euclidean distance between decision square and top right square\"\n",
    "        )\n",
    "    elif attr == \"steps_between_cheese_decision_square\":\n",
    "        return \"Steps between cheese and decision square\"\n",
    "    elif attr == \"euc_dist_cheese_decision_square\":\n",
    "        return \"Euclidean distance between cheese and decision square\"\n",
    "    elif attr == \"cheese_coord_norm\":\n",
    "        return \"Norm of cheese coordinate\"\n",
    "    elif attr == \"taxi_dist_cheese_decision_square\":\n",
    "        return \"Taxicab distance between cheese and decision square\"\n",
    "    elif attr == \"taxi_dist_cheese_top_right\":\n",
    "        return \"Taxicab distance between cheese and top right square\"\n",
    "    elif attr == \"taxi_dist_decision_square_top_right\":\n",
    "        return \"Taxicab distance between decision square and top right square\"\n",
    "    elif attr == \"taxi_dist_cheese_5x5\":\n",
    "        return \"Taxicab distance between cheese and top-right 5x5\"\n",
    "    elif attr == \"taxi_dist_decision_square_5x5\":\n",
    "        return \"Taxicab distance between decision square and top-right 5x5\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown attribute {attr}\")\n",
    "\n",
    "\n",
    "def format_attr(attr: str):\n",
    "    attr_str = english_attr(attr)\n",
    "    return bold_text(attr_str) if attr in claimed_attributes else attr_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04235c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name: str = \"model_rand_region_5\"\n",
    "files = glob(f\"experiments/statistics/data/{model_name}/*.pkl\")\n",
    "runs = []\n",
    "for f in files:\n",
    "    try:\n",
    "        runs.append(load_episode(f, load_venv=False))\n",
    "    except (AssertionError, KeyError) as e:\n",
    "        print(f\"Malformed file {f}: {e}\")\n",
    "        os.remove(f)\n",
    "\n",
    "print(f\"Loaded {len(runs)} runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151864ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "recorded_metrics = defaultdict(list)\n",
    "recorded_runs = []\n",
    "got_cheese = []\n",
    "for run in tqdm(runs):\n",
    "    g = run.grid()\n",
    "    if decision_square(g) is None or (g[-5:, -5:] == maze.CHEESE).any():\n",
    "        continue\n",
    "    for name, metric in metrics.items():\n",
    "        recorded_metrics[name].append(metric(g))\n",
    "    got_cheese.append(float(run.got_cheese))\n",
    "    recorded_runs.append(run)\n",
    "\n",
    "runs = recorded_runs\n",
    "del recorded_runs\n",
    "got_cheese = np.array(got_cheese)\n",
    "len(got_cheese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cb7919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to turn the metrics into a dataframe, so we have to convert them to numpy arrays\n",
    "for name, metric in recorded_metrics.items():\n",
    "    recorded_metrics[name] = np.array(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a207f4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = sum(got_cheese) / len(got_cheese)\n",
    "print(f\"P(get cheese | decision square, cheese not in top 5x5) = {prob:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f00443e8",
   "metadata": {},
   "source": [
    "We can explore the data itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9338e2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a plotly histogram, where you select which metric to display using a dropdown menu\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from ipywidgets import interact\n",
    "import math\n",
    "\n",
    "fig = go.FigureWidget()\n",
    "\n",
    "\n",
    "@interact\n",
    "def show_histogram(metric=list(recorded_metrics.keys())):\n",
    "    \"\"\"Show a histogram of the metric on this dataset.\"\"\"\n",
    "    fig.data = []\n",
    "\n",
    "    # Add a trace to fig\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=recorded_metrics[metric], histnorm=\"probability density\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Set the title\n",
    "    fig.update_layout(title_text=f\"Histogram of {english_attr(metric)}\")\n",
    "    # Set y axis label to probability density\n",
    "    fig.update_yaxes(title_text=\"Probability density\")\n",
    "    fig.update_xaxes(title_text=english_attr(metric))\n",
    "\n",
    "    # Automatically display updates to fig without having to call fig.show()\n",
    "    display.display(fig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "560a4e90",
   "metadata": {},
   "source": [
    "And see how the data points relate to each other. Many exhibit natural correlations from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18bc057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Dropdown, Checkbox\n",
    "\n",
    "scatter_distances_fig = go.FigureWidget()\n",
    "\n",
    "\n",
    "# Make plotly scatterplot comparing two metrics, to check for collinearity\n",
    "@interact\n",
    "def show_scatter(\n",
    "    metric1=Dropdown(\n",
    "        options=list(recorded_metrics.keys()),\n",
    "        value=\"euc_dist_cheese_decision_square\",\n",
    "    ),\n",
    "    metric2=Dropdown(\n",
    "        options=list(recorded_metrics.keys()),\n",
    "        value=\"steps_between_cheese_decision_square\",\n",
    "    ),\n",
    "):\n",
    "    \"\"\"Show a scatterplot of two metrics on this dataset.\"\"\"\n",
    "    data = pd.DataFrame(recorded_metrics)\n",
    "\n",
    "    # Plot the scatterplot\n",
    "    scatter_distances_fig.data = []\n",
    "    scatter_distances_fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=data[metric1], y=data[metric2], mode=\"markers\", name=\"runs\"\n",
    "        )\n",
    "    )\n",
    "    scatter_distances_fig.update_layout(\n",
    "        title_text=f\"{english_attr(metric1)} vs {english_attr(metric2)}\"\n",
    "    )\n",
    "    scatter_distances_fig.update_xaxes(title_text=english_attr(metric1))\n",
    "    scatter_distances_fig.update_yaxes(title_text=english_attr(metric2))\n",
    "    display.display(scatter_distances_fig)\n",
    "\n",
    "    # Draw a line of best fit\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(\n",
    "        data[metric1], data[metric2]\n",
    "    )\n",
    "    scatter_distances_fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=data[metric1],\n",
    "            y=slope * data[metric1] + intercept,\n",
    "            mode=\"lines\",\n",
    "            name=\"best fit\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Hide the legend\n",
    "    scatter_distances_fig.update_layout(showlegend=False)\n",
    "\n",
    "    # Print the correlation coefficient\n",
    "    print(f\"Correlation: {np.corrcoef(data[metric1], data[metric2])[0,1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d758e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the correlation matrix in plotly\n",
    "final_metrics = [\"euc_dist_cheese_decision_square\"]\n",
    "\n",
    "correlations = np.zeros((len(recorded_metrics), len(recorded_metrics)))\n",
    "for i, metric1 in enumerate(recorded_metrics.keys()):\n",
    "    for j, metric2 in enumerate(recorded_metrics.keys()):\n",
    "        correlations[i, j] = np.corrcoef(\n",
    "            recorded_metrics[metric1], recorded_metrics[metric2]\n",
    "        )[0, 1]\n",
    "\n",
    "# Show the correlation matrix in plotly, with a colorbar\n",
    "# On mouse over, show the name of each metric\n",
    "corrmap = px.imshow(\n",
    "    correlations,\n",
    "    labels=dict(x=\"Metric 1\", y=\"Metric 2\", color=\"Correlation\"),\n",
    "    color_continuous_scale=\"RdBu\",\n",
    "    zmin=-1,\n",
    "    zmax=1,\n",
    ")\n",
    "# Hover template: show the name of each metric, by looking up its value in the x and y lists\n",
    "corrmap.update_traces(hovertemplate=\"(%{x}, %{y}) = %{z:.3f} <extra></extra>\")\n",
    "\n",
    "corrmap.update_layout(title_text=\"Correlation matrix between metrics\")\n",
    "# Don't show numbers over each cell\n",
    "corrmap.update_traces(text=None)\n",
    "# Show x and y axis labels\n",
    "corrmap.update_xaxes(\n",
    "    ticktext=list(map(english_attr, recorded_metrics.keys())),\n",
    "    tickvals=list(range(len(recorded_metrics.keys()))),\n",
    ")\n",
    "\n",
    "corrmap.update_yaxes(\n",
    "    ticktext=list(map(english_attr, recorded_metrics.keys())),\n",
    "    tickvals=list(range(len(recorded_metrics.keys()))),\n",
    ")\n",
    "\n",
    "# Hide x and y axis titles\n",
    "corrmap.update_xaxes(title_text=\"\")\n",
    "corrmap.update_yaxes(title_text=\"\")\n",
    "\n",
    "# Hide the color bar\n",
    "corrmap.update_layout(coloraxis_showscale=False)\n",
    "\n",
    "# Make size of plot a bit bigger\n",
    "corrmap.update_layout(width=1000, height=1000)\n",
    "corrmap.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41d1c538",
   "metadata": {},
   "source": [
    "And here are the strongest correlations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8514cf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top k absolute value correlations, ignoring diagonals\n",
    "k = 50\n",
    "len_diagonal = len(correlations)\n",
    "topk = np.argsort(np.abs(correlations).flatten())[-(k + len_diagonal) :][\n",
    "    ::-1\n",
    "]  # Diagonal will be 1, so just add in len_diagonal\n",
    "\n",
    "# Print the top k correlations in a pretty table\n",
    "table = prettytable.PrettyTable()\n",
    "table.field_names = map(bold_text, [\"Metric 1\", \"Correlation\", \"Metric 2\"])\n",
    "\n",
    "for i in topk:\n",
    "    # Get the row and column of the correlation\n",
    "    row, col = i // len(correlations), i % len(correlations)\n",
    "    # Ignore the diagonal and the lower triangle\n",
    "    if row >= col:\n",
    "        continue\n",
    "    # Get the metric names\n",
    "    metric1, metric2 = (\n",
    "        list(recorded_metrics.keys())[row],\n",
    "        list(recorded_metrics.keys())[col],\n",
    "    )\n",
    "    # Add the row to the table\n",
    "    table.add_row(\n",
    "        [\n",
    "            format_attr(metric1),\n",
    "            f\"{correlations[row, col]:.3f}\",\n",
    "            format_attr(metric2),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51004a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_regression(attrs: List[str], data_frame: pd.DataFrame):\n",
    "    \"\"\"Runs a LASSO-regularized regression on the data using the given attributes. Returns the clf.\"\"\"\n",
    "    assert len(attrs) > 0, \"Must have at least one attribute to regress upon\"\n",
    "    for attr in attrs:\n",
    "        assert attr in data_frame, f\"Attribute {attr} not in data frame\"\n",
    "    assert \"cheese\" in data_frame, \"Attribute 'cheese' not in data frame\"\n",
    "\n",
    "    x = data_frame[attrs]\n",
    "    y = np.ravel(data_frame[[\"cheese\"]])\n",
    "\n",
    "    clf = LogisticRegression(\n",
    "        random_state=0, solver=\"liblinear\", penalty=\"l1\"\n",
    "    ).fit(x, y)\n",
    "    return clf\n",
    "\n",
    "\n",
    "def compute_avg_accuracy(\n",
    "    attrs: List[str], data_frame: pd.DataFrame, num_runs: int\n",
    ") -> float:\n",
    "    \"\"\"Runs a LASSO-regularized regression num_runs times on the data using the given attributes. Returns the average accuracy.\"\"\"\n",
    "    assert len(attrs) > 0, \"Must have at least one attribute to regress upon\"\n",
    "    assert num_runs > 0, \"Must run at least one time\"\n",
    "\n",
    "    accuracies = []\n",
    "    for i in range(num_runs):\n",
    "        train, test = train_test_split(data_frame, test_size=0.2)\n",
    "        clf = run_regression(attrs, train)\n",
    "        accuracies.append(clf.score(test[attrs], test[\"cheese\"]))\n",
    "    return np.mean(accuracies)\n",
    "\n",
    "\n",
    "def display_coeff_table(clf: Any, attrs: List[str]):\n",
    "    \"\"\"Displays the coefficients for each attribute, printing the label next to each coefficient.\"\"\"\n",
    "    assert len(attrs) > 0, \"Must have at least one attribute\"\n",
    "\n",
    "    # Print the coefficient for each attribute, printing the label next to each coefficient\n",
    "    table = prettytable.PrettyTable()\n",
    "    table.field_names = [bold_text(\"Attribute\"), bold_text(\"Coefficient\")]\n",
    "    for i, attr in enumerate(attrs):\n",
    "        table.add_row([format_attr(attr), f\"{clf.coef_[0][i]:.3f}\"])\n",
    "\n",
    "    # Add a row for the intercept\n",
    "    table.add_row([\"Intercept\", f\"{clf.intercept_[0]:.3f}\"])\n",
    "    print(table)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d5bf611e",
   "metadata": {},
   "source": [
    "# Using maze features to predict whether the mouse got the cheese "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3f5156",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(recorded_metrics.keys())\n",
    "data = {key: recorded_metrics[key] for key in keys}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df = stats.zscore(\n",
    "    df\n",
    ")  # zscore standardizes the data by subtracting the mean and dividing by the standard deviation\n",
    "\n",
    "# Now we want to add the cheese column to the dataframe\n",
    "df[\"cheese\"] = pd.DataFrame(\n",
    "    {\"cheese\": [(runs[i].got_cheese) for i in range(len(runs))]}\n",
    ")\n",
    "\n",
    "# Choose which keys to regress upon\n",
    "attributes = [\n",
    "    \"steps_between_cheese_5x5\",\n",
    "    \"euc_dist_cheese_5x5\",\n",
    "    \"steps_between_decision_square_5x5\",\n",
    "    \"euc_dist_decision_square_5x5\",\n",
    "    \"steps_between_cheese_top_right\",\n",
    "    \"euc_dist_cheese_top_right\",\n",
    "    \"steps_between_decision_square_top_right\",\n",
    "    \"euc_dist_decision_square_top_right\",\n",
    "    \"steps_between_cheese_decision_square\",\n",
    "    \"euc_dist_cheese_decision_square\",\n",
    "    \"cheese_coord_norm\",\n",
    "]\n",
    "\n",
    "n_runs = 50\n",
    "\n",
    "avg_accuracy = 0\n",
    "avg_coefficients = np.zeros(len(attributes) + 1)  # Add one for the intercept\n",
    "# We reduce variance in the score by running the regression multiple times\n",
    "for x in range(n_runs):\n",
    "    train, test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "    clf = run_regression(attributes, train)\n",
    "    avg_coefficients[:-1] = (\n",
    "        clf.coef_[0] + avg_coefficients[:-1]\n",
    "    )  # Update all but last entry with coeffs\n",
    "    avg_coefficients[-1] += clf.intercept_[0]  # Last entry is the intercept\n",
    "\n",
    "    x = test[attributes]\n",
    "    y = np.ravel(test[[\"cheese\"]])\n",
    "    avg_accuracy += clf.score(x, y)\n",
    "\n",
    "avg_accuracy /= n_runs\n",
    "avg_coefficients /= n_runs\n",
    "# Print the coefficient for each attribute, printing the label next to each coefficient (for the last run)\n",
    "display_coeff_table(clf, attributes)\n",
    "# print(avg_coefficients / n_runs) # TODO show what avg coefficients are\n",
    "print(\n",
    "    f\"The average regression accuracy is {avg_accuracy:.3f}, averaged over\"\n",
    "    f\" {n_runs} regressions.\"\n",
    ")\n",
    "\n",
    "# Record the sign of the coefficients for each attribute\n",
    "regression_coeff_signs = {\n",
    "    attr: clf.coef_[0][i] > 0\n",
    "    for i, attr in enumerate(attributes)\n",
    "    if attr in claimed_attributes\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8c909c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cheese_rate = sum(df[\"cheese\"]) / len(df[\"cheese\"])\n",
    "max_baseline = max(\n",
    "    cheese_rate, 1 - cheese_rate\n",
    ")  # Always predict \"cheese\" or \"no cheese\"\n",
    "print(\n",
    "    'Accuracy of the trivial \"always yes\" or \"always no\" predictor:'\n",
    "    f\" {max_baseline:.3f}\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19678d56",
   "metadata": {},
   "source": [
    "Let's examine whether it's predictively useful to know the Euclidean\n",
    "distance between the cheese and the decision square, given that we\n",
    "already know the step distance. (Spoiler: It is!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22f899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_lists = [\n",
    "    [\"steps_between_cheese_decision_square\"],\n",
    "    [\n",
    "        \"steps_between_cheese_decision_square\",\n",
    "        \"euc_dist_cheese_decision_square\",\n",
    "    ],\n",
    "]\n",
    "n_regressions = 100\n",
    "\n",
    "for attr_list in attr_lists:\n",
    "    print()\n",
    "    clf = run_regression(attr_list, df)\n",
    "    display_coeff_table(clf, attr_list)\n",
    "    print(\n",
    "        \"The accuracy for these attributes is\"\n",
    "        f\" {compute_avg_accuracy(attr_list, df, n_regressions):.3f}, averaged\"\n",
    "        f\" over {n_regressions} regressions.\"\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33762c04",
   "metadata": {},
   "source": [
    "# Stress-testing our results\n",
    "Overall, the features we claimed to be important, still seem to be important, although to different strengths. The claimed attributes don't exhibit sign-flips in a range of conditions.\n",
    "\n",
    "## Trying to re-pinpoint the claimed features\n",
    "Peli Grietzer originally found the bolded attributes using the following methodology, which he describes as follows:\n",
    "\n",
    "> I did multiple logistic regression with all the factors at once, then did a multiple logistic regression with all-factors-except-x for each x and wrote down which factors caused test accuracy loss when dropped.\n",
    ">\n",
    "> Four factors caused non-trivial test accuracy loss, so I took those four factors and did a multiple logistic regression on these four factors, and saw that the test accuracy was as good as with all factors.\n",
    ">\n",
    "> I then tested dropping each of the four factors and using just three, and saw that there was a non-trivial drop in test accuracy for each of them.\n",
    ">\n",
    "> I then tested adding one additional factor to the four factors, trying every unused factor and seeing no increase in test accuracy.\n",
    "\n",
    "This section implements that methodology. This doesn't seem to replicate, which is some evidence against our initial results -- there's less of a reason to pick out the four attributes we did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fda0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a multiple logistic regression with all-attributes-except-x for each x and store which attributes caused test accuracy loss when dropped.\n",
    "relative_drop = 0.005\n",
    "accuracy_drop_attrs = []\n",
    "num_runs = 5\n",
    "\n",
    "for attr in recorded_metrics.keys():\n",
    "    attr_accuracy = compute_avg_accuracy(\n",
    "        list(recorded_metrics.keys() - {attr}), df, num_runs\n",
    "    )\n",
    "    if attr_accuracy < avg_accuracy * (1 - relative_drop):\n",
    "        accuracy_drop_attrs.append(attr)\n",
    "\n",
    "print(\n",
    "    f\"When excluded, the following attributes caused a >{relative_drop*100}%\"\n",
    "    \" relative drop in accuracy:\"\n",
    ")\n",
    "for attr in accuracy_drop_attrs:\n",
    "    print(f\"\\t{format_attr(attr)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a3d5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take these attributes and run a multiple logistic regression on these attributes, and check that the test accuracy was as good as with all attributes.\n",
    "if len(accuracy_drop_attrs) > 0:\n",
    "    new_accuracy = compute_avg_accuracy(accuracy_drop_attrs, df, num_runs=10)\n",
    "    print(\n",
    "        \"The accuracy of the regression with the dropped attributes is\"\n",
    "        f\" {new_accuracy:.3f}.\"\n",
    "    )\n",
    "    if new_accuracy < avg_accuracy * (1 - relative_drop):\n",
    "        print(\n",
    "            \"Accuracy of regression with dropped attributes is not as good as\"\n",
    "            \" with all attributes.\"\n",
    "        )\n",
    "\n",
    "    # Try dropping each of the attributes that caused a drop in accuracy and verify that the accuracy drops significantly\n",
    "    for attr in accuracy_drop_attrs:\n",
    "        attr_accuracy = compute_avg_accuracy(\n",
    "            list(recorded_metrics.keys() - {attr}), df, num_runs=1\n",
    "        )\n",
    "        if attr_accuracy > avg_accuracy * (1 - relative_drop):\n",
    "            print(\n",
    "                f\"The accuracy of the regression with ({format_attr(attr)})\"\n",
    "                \" dropped is not significantly lower than with all attributes\"\n",
    "                f\" ({avg_accuracy:.3f}).\"\n",
    "            )\n",
    "    print()\n",
    "\n",
    "    # Try adding each of the attributes that did not cause a drop in accuracy and see if the accuracy increases significantly\n",
    "    for attr in list(recorded_metrics.keys() - set(accuracy_drop_attrs)):\n",
    "        attr_accuracy = compute_avg_accuracy(\n",
    "            list(recorded_metrics.keys() | {attr}), df, num_runs=1\n",
    "        )\n",
    "        if attr_accuracy > avg_accuracy * (1 - relative_drop):\n",
    "            print(\n",
    "                f\"The accuracy of the regression with ({format_attr(attr)})\"\n",
    "                \" added is not significantly higher than with all attributes.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927b82c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "claimed_accuracy = compute_avg_accuracy(claimed_attributes, df, num_runs=10)\n",
    "clf = run_regression(claimed_attributes, df)\n",
    "display_coeff_table(clf, claimed_attributes)\n",
    "print(\n",
    "    f\"Claimed attributes obtain accuracy of {claimed_accuracy:.3f}, which is\"\n",
    "    f\" {(avg_accuracy - claimed_accuracy)*100:.3f} absolute percent less than\"\n",
    "    \" when all features are used.\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "116e6e4c",
   "metadata": {},
   "source": [
    "## Robustness to regressing on random subsets of attributes\n",
    "\n",
    "When regressing on related factors (e.g. Euclidean and step distance to cheese from decision square), we might wonder: \"Did these coefficients only come out this way as a fluke of the regression itself?\". For example, factor subset X may induce feature $a$ to have a positive coefficient, while a different factor subset Y induces $a$ with a negative regression coefficient. Encouragingly, the signs of our claimed attributes are highly robust to which set of factors we regress upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224e12a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_subset_regression(\n",
    "    data_frame: pd.DataFrame, n_attrs: int, verbose: bool = False\n",
    ") -> Tuple[Any, List[str]]:\n",
    "    \"\"\"Runs a regression on the data frame using a random subset of n_attrs attributes. Returns the clf and the attributes used.\"\"\"\n",
    "    attrs = random.sample(list(recorded_metrics.keys() - {\"cheese\"}), n_attrs)\n",
    "    clf = run_regression(attrs, train)\n",
    "\n",
    "    if verbose:\n",
    "        display_coeff_table(clf, attrs)\n",
    "\n",
    "    return clf, attrs\n",
    "\n",
    "\n",
    "def check_claimed_signs(\n",
    "    clf: Any, attrs: List[str], data_frame: pd.DataFrame, verbose: bool = False\n",
    ") -> Dict[str, int]:\n",
    "    \"\"\"Checks if the signs of the regression coefficients for the given attributes match regression_coeff_signs. Returns a dictionary which counts the number of times each attribute had the wrong sign.\"\"\"\n",
    "    counters = defaultdict(int)\n",
    "\n",
    "    for i, attr in enumerate(attrs):\n",
    "        if attr not in regression_coeff_signs.keys() or attr in counters:\n",
    "            continue\n",
    "        assert attr in data_frame, f\"Attribute {attr} not in data frame\"\n",
    "        if (clf.coef_[0][i] >= 0) != regression_coeff_signs[attr]:\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"Attribute {attr} has incorrect sign; expected\"\n",
    "                    f\" {regression_coeff_signs[attr]} but got\"\n",
    "                    f\" {clf.coef_[0][i] >= 0}\"\n",
    "                )\n",
    "                display_coeff_table(clf, attrs)\n",
    "            counters[\n",
    "                attr\n",
    "            ] += 1  # Increment the attribute that had the incorrect sign\n",
    "    return counters\n",
    "\n",
    "\n",
    "# Run the regression multiple times and check the signs\n",
    "# See distribution of sign errors over multiple runs\n",
    "counter = {attr: 0 for attr in claimed_attributes}\n",
    "n_rand_runs = 200\n",
    "for x in range(n_rand_runs):\n",
    "    # Draw a random number of attributes to use\n",
    "    n_attrs = random.randint(1, len(recorded_metrics.keys()) - 1)\n",
    "    clf, attrs = run_subset_regression(train, n_attrs)\n",
    "    new_counter = check_claimed_signs(clf, attrs, train, verbose=False)\n",
    "    for key in new_counter.keys():\n",
    "        counter[key] += new_counter[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96cd3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a table displaying the percentage of times each attribute had the wrong sign\n",
    "sign_table = prettytable.PrettyTable()\n",
    "sign_table.field_names = [\"Attribute\", \"% of runs with flipped sign\"]\n",
    "for key in counter.keys():\n",
    "    sign_table.add_row([english_attr(key), counter[key] / n_rand_runs * 100])\n",
    "print(sign_table)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83f3df78",
   "metadata": {},
   "source": [
    "## Variation inflation factor (VIF)\n",
    "As I (Alex) understand it, VIF measures how much of the variance of the regression coefficients are due to multicollinearity. Restricting our attention to the features mentioned in the post, the VIF scores are medium-high (but tolerable), while including all features leads to extremely high VIF. This is some weak further evidence that our regression was not a statistical fluke or modeling decision (but I'm not a statistician!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba3bf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute VIF (variance inflation factor) for each attribute\n",
    "# This is a measure of how much the variance of the coefficient is inflated due to multicollinearity\n",
    "# Above 5 is usually considered to be a sign of that\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Ignore runtime warnings due to division by 0 from R2=1\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "for attrs in [claimed_attributes, recorded_metrics.keys()]:\n",
    "    print()\n",
    "    print(\n",
    "        \"VIF for claimed attributes\"\n",
    "        if attrs == claimed_attributes\n",
    "        else \"VIF for all attributes\"\n",
    "    )\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"Features\"] = df[attrs].columns\n",
    "    vif[\"VIF\"] = [\n",
    "        variance_inflation_factor(df[attrs].values, i)\n",
    "        for i in range(df[attrs].shape[1])\n",
    "    ]\n",
    "\n",
    "    print(vif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MATS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "4c77f2f46953a93e2cdf30c808e94602375d16ad6294e549473c1f301bc8b554"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
